{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4, Problem 2: Robot localization using particle filtering\n",
    "\n",
    "In the lecture we have learned particle filtering, which can be interpreted as a Monte Carlo method for Hidden Markov Models. In this exercise, we learn how to use this method to track a moving robot's position over time. In this setting, we have access to the steering and velocity control inputs. We also have sensors that measure the distances to visible landmarks. The basic principle of particle filtering is then that the population of particles tracks the high-likelihood regions of the robot's position.\n",
    "\n",
    "Your tasks is to complete the missing code. Make sure that all the functions follow the provided interfaces of the functions, i.e. the output of the function exactly matches the description in the docstring.\n",
    "Adding or modifying code outside of the following comment blocks is not required:\n",
    "\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook. You are **NOT** allowed to using additional `import`  statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# uncomment for multiple output in a cell\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize particles\n",
    "Since we don't have detailed information about the robot's starting position,\n",
    "particles are initialized by uniformly sampling in the 2D space defined by the width and height of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_particles_construction(width, height, N) -> np.ndarray:\n",
    "    \"\"\"Sample particles in 2D space with no landmarks randomly.\n",
    "\n",
    "    Args:\n",
    "        width, height: float, width (x) and height (y) of the area in which the robot moves\n",
    "        N: int, number of particles\n",
    "    Return:\n",
    "        particles: numpy (np) array with particles centered around the origin with dimension particles.size = [N,2] \n",
    "    \"\"\"\n",
    "\n",
    "    # setting the seed ensures drawing consistent samples for each execution of the particle filter.\n",
    "    # This can be changed for testing purposes and doesn't influence the result on Artemis.\n",
    "    np.random.seed(200)\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show a example map in which the robot moves:\n",
    "\n",
    "<img src=\"./data/PF_data/img/Map_Rejection_Sampling.png\" alt=\"Encoder\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Reject invalid particles\n",
    "The map above contains landmarks shown as blue circles.\n",
    "In this case, not all previous sampled particles are valid since these particles can not located inside the landmarks.\n",
    "In this task you need to find the valid particles, namely, only those particles whose distance to each landmarks'\n",
    "center is larger or equal to the landmarks' corresponding radius is considered as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_particles(particles, centers, radii):\n",
    "    \"\"\"Given randomly sampled particles, as well as centers and radii of landmarks, decide which particles are valid.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: coordinate for each centers of landmarks\n",
    "        radii: the radius of cicular landmarks \n",
    "    Return:\n",
    "        valid_particles: the particles that locate outside the circular landmarks\n",
    "    \"\"\"\n",
    "    valid_particles = []\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    radii=radii.reshape(-1)\n",
    "    for i, particle in enumerate(particles):\n",
    "        distance = np.linalg.norm(centers- particle, axis=1)\n",
    "        distance-=radii\n",
    "     #   print(distance)\n",
    "      #  print([distance>0])\n",
    "        if((distance>=0).all()):\n",
    "            valid_particles.append(particles[i])\n",
    "        \n",
    "                \n",
    "      \n",
    "    ###################################################################\n",
    "    return np.asarray(valid_particles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "particles=uniform_particles_construction(20,20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "particles=np.array([[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_valid_particles(particles,np.asarray([[2,2],[5,5]]),np.asarray([[2],[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Next, the model for propagating the particles' positions needs to be defined.\n",
    "Since we have access to the measured velocity and angle of the robot's current state,\n",
    "we can incorporate this information into the prediction.\n",
    "Furthermore, we add an uncertainty `std` (standard deviation) to account for the uncertain behavior in the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_particle_pos(particles, v, std):\n",
    "    \"\"\"Predict the position at the next time step for each particle given current angles and velocities.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejecting the ones that are not valid\n",
    "        v： array with current measurements [angle, velocity]\n",
    "        std: control the uncertainty of the prediction\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    dt = 0.4\n",
    "    # add some noise to the distance, the level is controlled by std\n",
    "    delta_dist = (v[1] * dt)\n",
    "    mean = (0, 0)\n",
    "\n",
    "    # covariance for tangential and orthogonal uncertainty of the motion\n",
    "    cov = [[std**2, 0], [0, (0.15*std)**2]]\n",
    "    x = np.random.multivariate_normal(mean, cov, N)\n",
    "\n",
    "    # update the positions of all particles\n",
    "   \n",
    "    particles[:, 0] += np.cos(v[0]) * (delta_dist + x[:,0]) - np.sin(v[0]) * x[:,1]\n",
    "    particles[:, 1] += np.sin(v[0]) * (delta_dist + x[:,0]) + np.cos(v[0]) * x[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Update the weights of each particle\n",
    "Update the weights of the particles based on the measured distances to the landmarks.\n",
    "The weight denotes how well the sensor model can explain the deviation between the actual measurements and the distances to the landmarks taken from the estimated position.\n",
    "The weights are normalized in order to obtain a probability distribution.\n",
    "Those particles that are closer to the actual position of the robot will generally have a higher weight than those far away from the robot.\n",
    "\n",
    "In this task, we use a normal probability distribution for the sensor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_update(particles, weights, dist_r_l, centers, radii, std_sensor):\n",
    "    \"\"\"Given the noised distances between robot and the landmarks, update the weights of particles\n",
    "        \n",
    "    Args:\n",
    "        particles: coordinate of particles, shape = [N,2]\n",
    "        weights: weights of particles, shape = [N,1]\n",
    "        dist_r_l: the current distance between robot and landmarks\n",
    "        centers: coordinate for each center of the landmarks\n",
    "        radii: the radius of the circular landmarks\n",
    "        std_sensor: standard deviation of the normal distribution that represents the sensor model\n",
    "    \n",
    "    Return:\n",
    "        weights: array with updated weights for each particle, shape = [N,1]\n",
    "        \n",
    "    \"\"\"\n",
    "    weights.fill(1.)\n",
    "    \n",
    "    \n",
    "    # in order to avoid round-off to zero, we add eps to each weight before multiplying it\n",
    "    # e.g.: weights *= weight + eps\n",
    "    eps = 1.e-15   \n",
    "  #  weights_i=copy.deepcopy(weights)\n",
    "\n",
    "  \n",
    "   # print(particles.shape)# 600 2\n",
    "    #print(weights.shape)# 600 1\n",
    "    #print(dist_r_l.shape)# 10 1\n",
    "    #print(centers.shape)# 10 2\n",
    "    #print(radii.shape) 10 1\n",
    "    \n",
    "    for count, center in enumerate(centers):\n",
    "        ###################################################################\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # weights_i = # to be computed\n",
    "        dist = np.linalg.norm(particles - center, axis=1)-radii[count] # 576\n",
    "        weights_i = stats.norm(dist, std_sensor).pdf(dist_r_l[count]).reshape(-1,1)\n",
    "    #    print(sum(weights_i))\n",
    "      #  weights_i /= sum(weights_i)\n",
    "      #  weights_i[dist<radii[count]]=0\n",
    "    #    print((weights_i))\n",
    "        #weights[dist<radii[count]]=0\n",
    "      #  weights_i /= sum(weights_i)\n",
    "      #  print(sum(weights_i))\n",
    "    #    weights /= sum(weights)\n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################################################\n",
    "        # adding eps to avoid numerical issues\n",
    "        weights *=  weights_i + eps\n",
    "        weights /= sum(weights)\n",
    "        \n",
    "  #  print(weights)\n",
    "    weights += eps\n",
    "    # normalize weights\n",
    "    weights /= sum(weights) + eps\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Resample Procedure\n",
    "Resample the next set of particles from the previously propagated set of particles.\n",
    "The probability of drawing a particle is based on its previously computed weight.\n",
    "In principle, there are many ways to achieve this,\n",
    "here you can refer to the procedure given in the paper [Resampling in Particle Filtering - Comparison](http://sait.cie.put.poznan.pl/38/SAIT_38_02.pdf)\n",
    "to complete the systematic resampling method, which is summarized below where particles are denoted by\n",
    "$ \\mathbf{x}$ and weights are denoted by $\\mathbf{q}$:\n",
    "\n",
    "<img src=\"./data/PF_data/img/systematic_resample.png\" alt=\"Encoder\" style=\"width: 400px;\"/>\n",
    "\n",
    "This approach subdivides that the range of the weights $[0, 1)$ into N equally sized intervals,\n",
    "and then draws samples within each interval $u^i \\sim [\\frac{i-1}{N}, \\frac{i}{N})$.\n",
    "Particles are selected for replication, such that $u^i \\in [\\sum_p^{j-1}q_p, \\sum_p^{j}q_p)$.\n",
    "This resampling algorithm has a complexity of *O(N)*, and is one of the more readily recommended, due to its simplicity and operation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_procedure(x, weights, u0=np.random.uniform()):\n",
    "    \"\"\"Perform the resampling procedure described above\n",
    "\n",
    "    Args:\n",
    "        x: particles from which to resample, shape = [N,2]\n",
    "        weights: the weights of the particles\n",
    "        u0: random number which is drawn only once\n",
    "\n",
    "    Return:\n",
    "        xx: the resampled particles\n",
    "        idx: indices of the particles in the resampled array xx (same order as in xx)\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "\n",
    "    u = u0 / N\n",
    "    xx = np.zeros_like(x)\n",
    "    idx = np.zeros(N, dtype='int')\n",
    "    sumQ = copy.copy(weights[0])\n",
    "    j=0\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    for i in range(N):\n",
    "        while sumQ<u and j+1<N:\n",
    "            j+=1\n",
    "            sumQ+=weights[j]\n",
    "        xx[i]=x[j]\n",
    "        idx[i]=j\n",
    "        u=u+1/N\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################################################################\n",
    "    return xx, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the indices of the resampled particles, we need to update the weights as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_resampled_weights(weights, idx):\n",
    "    weights[:] = weights[idx]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Here we are provided with information about landmarks of the maps, and also the robot's trajectory as reference.\n",
    "The velocity and distance data can be interpreted as the observed states in the HMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_centers = np.load('./data/PF_data/centers/example_centers.npy')\n",
    "landmark_radii = np.load('./data/PF_data/radii/example_radii.npy')\n",
    "\n",
    "angle_velocity = np.load('./data/PF_data/velocity/example_velocity.npy')\n",
    "dist_r_l = np.load('./data/PF_data/distance/example_distance.npy')\n",
    "\n",
    "# only for plotting - you should not touch this data in your implementation\n",
    "input_sequence = np.load('./data/PF_data/trajectory/example_trajectory.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to see how a robot moves on a new map (doesn't influence results on Artemis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_ID = 0  # choose between 0 and 1\n",
    "# landmark_centers = np.load(f'./data/PF_data/centers/centers_{map_ID}.npy')\n",
    "# landmark_radii = np.load(f'./data/PF_data/radii/radii_{map_ID}.npy')\n",
    "\n",
    "# angle_velocity = np.load(f'./data/PF_data/velocity/velocity_{map_ID}.npy')\n",
    "# dist_r_l = np.load(f'./data/PF_data/distance/distance_{map_ID}.npy')\n",
    "\n",
    "# # only for plotting - you should not touch this data in your implementation\n",
    "# input_sequence = np.load(f'./data/PF_data/trajectory/trajectory_{map_ID}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add noise to the sensor data to simulate imperfect real-world measurements (doesn't influence results on Artemis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_noise = 3.0\n",
    "angle_noise = 0.05\n",
    "velocity_noise = 20.0\n",
    "dist_r_l += np.random.normal(0.0, distance_noise, size=dist_r_l.shape)\n",
    "angle_velocity[:,0] += np.random.normal(0.0, angle_noise, size=angle_velocity[:,0].shape)\n",
    "angle_velocity[:,1] += np.random.normal(0.0, velocity_noise, size=angle_velocity[:,1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "In this cell we set some predefined hyperparameters. The `uncertainty_motion_model` contains different levels for the uncertainty in the motion model.\n",
    "`std_sensor` denotes the standard deviation of the distance measurements of the sensor.\n",
    "In this setting, we have continuous transition and sensor models in contrast to discrete models as in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_max = 800\n",
    "height_max = 600 \n",
    "particle_num = 600\n",
    "\n",
    "uncertainty_motion_model = {'no noise': 1.0, 'low noise': 7.0, 'high noise': 12.0}\n",
    "\n",
    "std_sensor = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Main particle filtering algorithm\n",
    "Finally, the implemented functions need to be put together in order to perform particle filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filtering(width, height, N, centers, radii, velocity, distance, std_prediction, std_sensor):\n",
    "    \"\"\" Main method for particle filtering. Returns the estimated trajectories of N particles.\n",
    "        \n",
    "    Args:\n",
    "        width, height: decide moving regions of the robot\n",
    "        N: number of particles\n",
    "        centers: centers of landmarks\n",
    "        radii: radii of landmarks\n",
    "        distance: measured distances from the robot to the landmarks\n",
    "        std_prediction: uncertainty used for prediction\n",
    "        std_sensor: standard deviation of the sensor model\n",
    "    Return:\n",
    "        list of positions of all particles, shape = [len(distance), N, 2],\n",
    "        list particle weights, shape = [len(distance), N]\n",
    "        final weights of particles, shape = [len(distance), 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # First we initialize the particles and the particle weights\n",
    "    random_particles = uniform_particles_construction(width, height, N)\n",
    "    particle_pos = find_valid_particles(random_particles, centers, radii)\n",
    "    particle_weights = np.ones((len(particle_pos),1))\n",
    "\n",
    "    # Now we need to record the coordinates of moving particles\n",
    "    particle_trajectories = [copy.copy(particle_pos)]\n",
    "    estimate_positions = []\n",
    "\n",
    "    for t in range(len(distance)-1):\n",
    "        ###################################################################\n",
    "        # YOUR CODE HERE\n",
    "        # find_valid_particles(particles, centers, radii)\n",
    "       # print(std_prediction)\n",
    "     #   print(t)\n",
    "     #   particle_pos=find_valid_particles(particle_pos, centers, radii)\n",
    "        propagate_particle_pos(particle_pos, velocity[t], std_prediction)\n",
    "        particle_weights=weights_update(particle_pos, particle_weights, distance[t+1], centers, radii, std_sensor)\n",
    "        \n",
    "        \n",
    "        particle_pos, random_index=resample_procedure(particle_pos, particle_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################################################\n",
    "\n",
    "        update_resampled_weights(particle_weights, random_index)\n",
    "        particle_trajectories.append(copy.copy(particle_pos))\n",
    "        estimate_positions.append(np.average(particle_pos, weights=particle_weights.flatten(), axis=0))\n",
    "\n",
    "    particle_trajectory = np.asarray(particle_trajectories)\n",
    "    estimate_positions = np.asarray(estimate_positions)\n",
    "    return particle_trajectory, estimate_positions, particle_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_result(width, height, N, centers, radii, v, dist, noise, std_sensor):\n",
    "    trajectory_all = []\n",
    "    weight_all = []\n",
    "    estimate_positions_all = []\n",
    "\n",
    "    for noise_i in noise.values():\n",
    "        trajectories, estimate_positions, weights = particle_filtering(width, height, N, centers, radii, v, dist, noise_i, std_sensor)\n",
    "        trajectory_all.append(trajectories)\n",
    "        estimate_positions_all.append(estimate_positions)\n",
    "        weight_all.append(weights)\n",
    "\n",
    "    return trajectory_all, weight_all, estimate_positions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_all, weight_all, estimate_positions = concatenate_result(width_max, height_max, particle_num, landmark_centers, landmark_radii, angle_velocity, dist_r_l, uncertainty_motion_model, std_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Now we visualize the ground truth movement of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "def location(time_step):\n",
    "    fig,ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    vmax=np.max(np.concatenate([w.flatten() for w in weight_all]))\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        ax_unpack = ax.ravel()\n",
    "        ax_unpack[i].set_title(f\"Transition model uncertainty: {list(uncertainty_motion_model.values())[i]}\")\n",
    "        ax_unpack[i].set_aspect('equal')\n",
    "        ax_unpack[i].plot(estimate_positions[i][:time_step,0],\n",
    "                          estimate_positions[i][:time_step,1], lw=1, c='b')\n",
    "\n",
    "        ax_unpack[i].plot(input_sequence[:time_step + 1,0], input_sequence[:time_step + 1, 1], lw=1, c='r')\n",
    "        ax_unpack[i].scatter(input_sequence[time_step, 0], input_sequence[time_step, 1], s=25, c='r', marker='x')\n",
    "        order = np.argsort(weight_all[i].flatten()) # plot highly weighted points on top\n",
    "        ax_unpack[i].scatter(trajectory_all[i][time_step, order, 0], trajectory_all[i][time_step, order, 1],s=2,\n",
    "                             c=weight_all[i].flatten()[order], cmap=cm.winter, vmin=0.0, vmax=vmax)\n",
    "\n",
    "        for count, value in enumerate(landmark_centers):\n",
    "            circ = mpatches.Circle(value,landmark_radii[count], alpha=0.5)\n",
    "            ax_unpack[i].add_patch(circ)\n",
    "        ax_unpack[i].imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legend\n",
    "**red**: ground truth positions\n",
    "\n",
    "**blue**: weighted average of the particle's positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c628ff073a848d3bf87ba2d77825d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='time_step', max=48), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = plt.imread('data/PF_data/img/Canvas.png')\n",
    "\n",
    "iplot = ipywidgets.interactive(location, time_step=(0, len(trajectory_all[1])-2))\n",
    "iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the result\n",
    "If your implementation is correct, the particles will eventually converge to the ground truth trajectory (red).\n",
    "As you can see, the results strongly depend on the uncertainty in the transition model.\n",
    "### Questions for yourself:\n",
    "- Why does a model with larger uncertainties often outperform the model with the smallest uncertainty?\n",
    "- How could this particle filter be improved? Also, try to play with the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
